\chapter{Chapter}
\section{Section}
\subsection{Math Comments}

Add all custom comment to the variables file
\begin{align}
\KL{\der{}{\alpha}f(x)}{p_\sigma}
\end{align}

$\X$


\subsection{Refernces}

Chapter reference: \autoref{ch: experiments}

Cite: \cite{MNIST}


\subsection{More Math}

\begin{theorem}[runtime of learning a neural network]
	Let $k \geq 3$. For every $n$, let $(V,E)$ be a layered graph with $n$ input nodes, $k + 1$ nodes at the (single) hidden layer, where one of them is the constant neuron, and a single output node. Then, it is NP-hard to implement the ERM rule with respect to $\mathcal{H}_{V_n,E_n,sign}$.
\end{theorem}

\begin{proof}
	Well let the supervisor to that...
\end{proof}

(similar setup for definitions and so on)